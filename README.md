# mgmt590-lec1

1) An automated question answering model which requeries "question" and "context" as input and gives "answer" as output. 
2) Pre-Trained Model used is "bert-base-multilingual-cased" from Hugging face Transformer Library.
3) Architecture of the model - 12-layer, 768-hidden, 12-heads, 179M parameters. 
4) Reason for using this model is that it is pre trained on multi lingual language, so it can help the client to scale the project across diff geographical location
5) Enjoy the code :) 
